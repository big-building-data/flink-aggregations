
# Kakfa
# =====
# configuration for communicating with Kafka
#
# the brokers address:port, comma-separated
kafka.brokers=localhost:6667
# the source topic, with the augmented values
kafka.augmentation=bbdata2-augmented
# (optional) the consumer group to use
# if you run multiple instances of this application (one for each aggregation granularity),
# ensure you give a different consumer group for each application !
kafka.consumer.group=flink-aggregation-01

# Cassandra
# =========
# configuration for saving aggregations to cassandra.
# the cassandra instance should have a table bbdata2.aggregations table !
#
# the cassandra entrypoints, a list of ip addresses separated by comma
cassandra.entryPoints=

# Flink checkpoints
# =================
# configure checkpointing
#
# the checkpoint interval
flink.checkpoints.interval=30000
# wether or not to use the externalized checkpoints (Flink 1.2+)
flink.checkpoints.externalized=true

# Window aggregations
# ===================
# configure the windowing system
#
# the granularity, in minutes. 15 for quarters of hour, 60 for aggregations per hour, etc.
window.granularity=15
# how many minutes a measure can be late in order to be considered "on-time". Note that the longer the
# allowed lateness, the longer a window stays in memory.
# If you have a window granularity and the allowed lateness in 30 minutes, then the process will keep the last
# two closed windows around --> there will be 3 windows in memory at any given time.
window.allowed_lateness=5
# how long do we keep windows open when no new record is submitted for a given object. This is useful in case
# a source stops sending values. In this case, we don't want the windows to stay open indefinitely.
# Thus, the process will remember the last _processing time_ it processed a value and after "timeout" without new
# measures it will flush all windows still in memory.
window.timeout=20

